{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    from torch.hub import load_state_dict_from_url  # noqa: F401\n",
        "except ImportError:\n",
        "    from torch.utils.model_zoo import load_url as load_state_dict_from_url  # noqa: F401"
      ],
      "metadata": {
        "id": "V6noNOq_LPPc"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import traceback\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "__all__ = [\n",
        "    \"ResNet\",\n",
        "    \"resnet18\",\n",
        "    \"resnet34\",\n",
        "    \"resnet50\",\n",
        "    \"resnet101\",\n",
        "    \"resnet152\",\n",
        "    \"resnext50_32x4d\",\n",
        "    \"resnext101_32x8d\",\n",
        "    \"wide_resnet50_2\",\n",
        "    \"wide_resnet101_2\",\n",
        "]\n",
        "\n",
        "\n",
        "model_urls = {\n",
        "    \"resnet18\": \"https://download.pytorch.org/models/resnet18-5c106cde.pth\",\n",
        "    \"resnet34\": \"https://download.pytorch.org/models/resnet34-333f7ec4.pth\",\n",
        "    \"resnet50\": \"https://download.pytorch.org/models/resnet50-19c8e357.pth\",\n",
        "    \"resnet101\": \"https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\",\n",
        "    \"resnet152\": \"https://download.pytorch.org/models/resnet152-b121ed2d.pth\",\n",
        "    \"resnext50_32x4d\": \"https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth\",\n",
        "    \"resnext101_32x8d\": \"https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth\",\n",
        "    \"wide_resnet50_2\": \"https://download.pytorch.org/models/wide_resnet50_2-95faca4d.pth\",\n",
        "    \"wide_resnet101_2\": \"https://download.pytorch.org/models/wide_resnet101_2-32ee1156.pth\",\n",
        "}\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(\n",
        "        in_planes,\n",
        "        out_planes,\n",
        "        kernel_size=3,\n",
        "        stride=stride,\n",
        "        padding=dilation,\n",
        "        groups=groups,\n",
        "        bias=False,\n",
        "        dilation=dilation,\n",
        "    )\n",
        "\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "    __constants__ = [\"downsample\"]\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        inplanes,\n",
        "        planes,\n",
        "        stride=1,\n",
        "        downsample=None,\n",
        "        groups=1,\n",
        "        base_width=64,\n",
        "        dilation=1,\n",
        "        norm_layer=None,\n",
        "    ):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        if groups != 1 or base_width != 64:\n",
        "            raise ValueError(\"BasicBlock only supports groups=1 and base_width=64\")\n",
        "        if dilation > 1:\n",
        "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
        "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = norm_layer(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "    __constants__ = [\"downsample\"]\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        inplanes,\n",
        "        planes,\n",
        "        stride=1,\n",
        "        downsample=None,\n",
        "        groups=1,\n",
        "        base_width=64,\n",
        "        dilation=1,\n",
        "        norm_layer=None,\n",
        "    ):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        width = int(planes * (base_width / 64.0)) * groups\n",
        "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv1x1(inplanes, width)\n",
        "        self.bn1 = norm_layer(width)\n",
        "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
        "        self.bn2 = norm_layer(width)\n",
        "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
        "        self.bn3 = norm_layer(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        block,\n",
        "        layers,\n",
        "        num_classes=1000,\n",
        "        zero_init_residual=False,\n",
        "        groups=1,\n",
        "        width_per_group=64,\n",
        "        replace_stride_with_dilation=None,\n",
        "        norm_layer=None,\n",
        "        in_channels=3,\n",
        "    ):\n",
        "        super(ResNet, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        self._norm_layer = norm_layer\n",
        "\n",
        "        self.inplanes = 64\n",
        "        self.dilation = 1\n",
        "        if replace_stride_with_dilation is None:\n",
        "            # each element in the tuple indicates if we should replace\n",
        "            # the 2x2 stride with a dilated convolution instead\n",
        "            replace_stride_with_dilation = [False, False, False]\n",
        "        if len(replace_stride_with_dilation) != 3:\n",
        "            raise ValueError(\n",
        "                \"replace_stride_with_dilation should be None \"\n",
        "                \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation)\n",
        "            )\n",
        "        self.groups = groups\n",
        "        self.base_width = width_per_group\n",
        "\n",
        "        # NOTE: strictly set the in_channels = 3 to load the pretrained model\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False\n",
        "        )\n",
        "        # self.conv1 = nn.Conv2d(in_channels, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = norm_layer(self.inplanes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(\n",
        "            block, 128, layers[1], stride=2, dilate=replace_stride_with_dilation[0]\n",
        "        )\n",
        "        self.layer3 = self._make_layer(\n",
        "            block, 256, layers[2], stride=2, dilate=replace_stride_with_dilation[1]\n",
        "        )\n",
        "        self.layer4 = self._make_layer(\n",
        "            block, 512, layers[3], stride=2, dilate=replace_stride_with_dilation[2]\n",
        "        )\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        # NOTE: strictly set the num_classes = 1000 to load the pretrained model\n",
        "        self.fc = nn.Linear(512 * block.expansion, 1000)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        # Zero-initialize the last BN in each residual branch,\n",
        "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
        "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, Bottleneck):\n",
        "                    nn.init.constant_(m.bn3.weight, 0)\n",
        "                elif isinstance(m, BasicBlock):\n",
        "                    nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
        "        norm_layer = self._norm_layer\n",
        "        downsample = None\n",
        "        previous_dilation = self.dilation\n",
        "        if dilate:\n",
        "            self.dilation *= stride\n",
        "            stride = 1\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
        "                norm_layer(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(\n",
        "            block(\n",
        "                self.inplanes,\n",
        "                planes,\n",
        "                stride,\n",
        "                downsample,\n",
        "                self.groups,\n",
        "                self.base_width,\n",
        "                previous_dilation,\n",
        "                norm_layer,\n",
        "            )\n",
        "        )\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(\n",
        "                block(\n",
        "                    self.inplanes,\n",
        "                    planes,\n",
        "                    groups=self.groups,\n",
        "                    base_width=self.base_width,\n",
        "                    dilation=self.dilation,\n",
        "                    norm_layer=norm_layer,\n",
        "                )\n",
        "            )\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "def _resnet(arch, block, layers, pretrained, progress, **kwargs):\n",
        "    model = ResNet(block, layers, **kwargs)\n",
        "    if pretrained:\n",
        "        state_dict = load_state_dict_from_url(model_urls[arch], progress=progress)\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet18(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"ResNet-18 model from\n",
        "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    model = _resnet(\n",
        "        \"resnet18\", BasicBlock, [2, 2, 2, 2], pretrained, progress, **kwargs\n",
        "    )\n",
        "\n",
        "    # model.fc = nn.Linear(512, kwargs['num_classes'])\n",
        "    model.fc = nn.Linear(512, 7)\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet34(pretrained=True, progress=True, **kwargs):\n",
        "    r\"\"\"ResNet-34 model from\n",
        "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    model = _resnet(\n",
        "        \"resnet34\", BasicBlock, [3, 4, 6, 3], pretrained, progress, **kwargs\n",
        "    )\n",
        "    model.fc = nn.Linear(512, kwargs[\"num_classes\"])\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet50(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"ResNet-50 model from\n",
        "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    model = _resnet(\n",
        "        \"resnet50\", Bottleneck, [3, 4, 6, 3], pretrained, progress, **kwargs\n",
        "    )\n",
        "    model.fc = nn.Linear(2048, kwargs[\"num_classes\"])\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet101(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"ResNet-101 model from\n",
        "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    model = _resnet(\n",
        "        \"resnet101\", Bottleneck, [3, 4, 23, 3], pretrained, progress, **kwargs\n",
        "    )\n",
        "    model.fc = nn.Linear(2048, kwargs[\"num_classes\"])\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet152(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"ResNet-152 model from\n",
        "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    model = _resnet(\n",
        "        \"resnet152\", Bottleneck, [3, 8, 36, 3], pretrained, progress, **kwargs\n",
        "    )\n",
        "\n",
        "    model.fc = nn.Linear(2048, kwargs[\"num_classes\"])\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnext50_32x4d(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"ResNeXt-50 32x4d model from\n",
        "    `\"Aggregated Residual Transformation for Deep Neural Networks\" <https://arxiv.org/pdf/1611.05431.pdf>`_\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    kwargs[\"groups\"] = 32\n",
        "    kwargs[\"width_per_group\"] = 4\n",
        "    return _resnet(\n",
        "        \"resnext50_32x4d\", Bottleneck, [3, 4, 6, 3], pretrained, progress, **kwargs\n",
        "    )\n",
        "\n",
        "\n",
        "def resnext101_32x8d(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"ResNeXt-101 32x8d model from\n",
        "    `\"Aggregated Residual Transformation for Deep Neural Networks\" <https://arxiv.org/pdf/1611.05431.pdf>`_\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    kwargs[\"groups\"] = 32\n",
        "    kwargs[\"width_per_group\"] = 8\n",
        "    return _resnet(\n",
        "        \"resnext101_32x8d\", Bottleneck, [3, 4, 23, 3], pretrained, progress, **kwargs\n",
        "    )\n",
        "\n",
        "\n",
        "def wide_resnet50_2(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"Wide ResNet-50-2 model from\n",
        "    `\"Wide Residual Networks\" <https://arxiv.org/pdf/1605.07146.pdf>`_\n",
        "\n",
        "    The model is the same as ResNet except for the bottleneck number of channels\n",
        "    which is twice larger in every block. The number of channels in outer 1x1\n",
        "    convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048\n",
        "    channels, and in Wide ResNet-50-2 has 2048-1024-2048.\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    kwargs[\"width_per_group\"] = 64 * 2\n",
        "    return _resnet(\n",
        "        \"wide_resnet50_2\", Bottleneck, [3, 4, 6, 3], pretrained, progress, **kwargs\n",
        "    )\n",
        "\n",
        "\n",
        "def wide_resnet101_2(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"Wide ResNet-101-2 model from\n",
        "    `\"Wide Residual Networks\" <https://arxiv.org/pdf/1605.07146.pdf>`_\n",
        "\n",
        "    The model is the same as ResNet except for the bottleneck number of channels\n",
        "    which is twice larger in every block. The number of channels in outer 1x1\n",
        "    convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048\n",
        "    channels, and in Wide ResNet-50-2 has 2048-1024-2048.\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    kwargs[\"width_per_group\"] = 64 * 2\n",
        "    return _resnet(\n",
        "        \"wide_resnet101_2\", Bottleneck, [3, 4, 23, 3], pretrained, progress, **kwargs\n",
        "    )\n",
        "\n",
        "def transpose(in_channels, out_channels, kernel_size=2, stride=2):\n",
        "    return nn.Sequential(\n",
        "        nn.ConvTranspose2d(\n",
        "            in_channels, out_channels, kernel_size=kernel_size, stride=stride\n",
        "        ),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(inplace=True),\n",
        "    )\n",
        "\n",
        "\n",
        "def downsample(in_channels, out_channels):\n",
        "    return nn.Sequential(\n",
        "        conv1x1(in_channels, out_channels),\n",
        "        nn.BatchNorm2d(num_features(out_channels)),\n",
        "        nn.ReLU(inplace=True),\n",
        "    )\n",
        "\n",
        "\n",
        "class Attention0(nn.Module):\n",
        "    def __init__(self, channels, block):\n",
        "        super().__init__()\n",
        "        self._trunk1 = block(channels, channels)\n",
        "        self._trunk2 = block(channels, channels)\n",
        "\n",
        "        self._enc = block(channels, channels)\n",
        "        self._dec = block(channels, channels)\n",
        "\n",
        "        self._conv1x1 = nn.Sequential(\n",
        "            conv1x1(2 * channels, channels),\n",
        "            nn.BatchNorm2d(num_features=channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "        self._mp = nn.MaxPool2d(3, 2, 1)\n",
        "        self._relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def enc(self, x):\n",
        "        return self._enc(x)\n",
        "\n",
        "    def dec(self, x):\n",
        "        return self._dec(x)\n",
        "\n",
        "    def trunking(self, x):\n",
        "        return self._trunk2(self._trunk1(x))\n",
        "\n",
        "    def masking(self, x):\n",
        "        x = self.enc(x)\n",
        "        x = self.dec(x)\n",
        "        return torch.sigmoid(x)\n",
        "\n",
        "    def forward(self, x):\n",
        "        trunk = self.trunking(x)\n",
        "        mask = self.masking(x)\n",
        "        return (1 + mask) * trunk\n",
        "\n",
        "\n",
        "class Attention1(nn.Module):\n",
        "    def __init__(self, channels, block):\n",
        "        super().__init__()\n",
        "        self._trunk1 = block(channels, channels)\n",
        "        self._trunk2 = block(channels, channels)\n",
        "\n",
        "        self._enc1 = block(channels, channels)\n",
        "        self._enc2 = block(channels, channels)\n",
        "\n",
        "        self._dec = block(channels, channels)\n",
        "        self._conv1x1 = nn.Sequential(\n",
        "            conv1x1(2 * channels, channels),\n",
        "            nn.BatchNorm2d(num_features=channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "        self._trans = nn.Sequential(\n",
        "            nn.ConvTranspose2d(channels, channels, kernel_size=2, stride=2),\n",
        "            nn.BatchNorm2d(num_features=channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "        self._mp = nn.MaxPool2d(3, 2, 1)\n",
        "        self._relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def enc(self, x):\n",
        "        x1 = self._enc1(x)\n",
        "        x2 = self._enc2(self._mp(x1))\n",
        "        return [x1, x2]\n",
        "\n",
        "    def dec(self, x):\n",
        "        x1, x2 = x\n",
        "        x2 = self._trans(x2)\n",
        "        x = torch.cat([x1, x2], dim=1)\n",
        "        x = self._conv1x1(x)\n",
        "        return self._dec(x)\n",
        "\n",
        "    def trunking(self, x):\n",
        "        return self._trunk2(self._trunk1(x))\n",
        "\n",
        "    def masking(self, x):\n",
        "        x = self.enc(x)\n",
        "        x = self.dec(x)\n",
        "        return torch.sigmoid(x)\n",
        "\n",
        "    def forward(self, x):\n",
        "        trunk = self.trunking(x)\n",
        "        mask = self.masking(x)\n",
        "        return (1 + mask) * trunk\n",
        "\n",
        "\n",
        "class Attention2(nn.Module):\n",
        "    def __init__(self, channels, block):\n",
        "        super().__init__()\n",
        "        self._trunk1 = block(channels, channels)\n",
        "        self._trunk2 = block(channels, channels)\n",
        "\n",
        "        self._enc1 = block(channels, channels)\n",
        "        self._enc2 = block(channels, channels)\n",
        "        self._enc3 = nn.Sequential(block(channels, channels), block(channels, channels))\n",
        "\n",
        "        self._dec1 = nn.Sequential(\n",
        "            conv1x1(2 * channels, channels),\n",
        "            nn.BatchNorm2d(num_features=channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            block(channels, channels),\n",
        "        )\n",
        "        self._dec2 = nn.Sequential(\n",
        "            conv1x1(2 * channels, channels),\n",
        "            nn.BatchNorm2d(num_features=channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            block(channels, channels),\n",
        "        )\n",
        "\n",
        "        self._trans = nn.Sequential(\n",
        "            nn.ConvTranspose2d(channels, channels, kernel_size=2, stride=2),\n",
        "            nn.BatchNorm2d(num_features=channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "        self._mp = nn.MaxPool2d(3, 2, 1)\n",
        "        self._relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        # ''' try to open this line and see the change of acc\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, Bottleneck):\n",
        "                nn.init.constant_(m.bn3.weight, 0)\n",
        "            elif isinstance(m, BasicBlock):\n",
        "                nn.init.constant_(m.bn2.weight, 0)\n",
        "        # '''\n",
        "\n",
        "    def enc(self, x):\n",
        "        x1 = self._enc1(x)\n",
        "        x2 = self._enc2(self._mp(x1))\n",
        "        x3 = self._enc3(self._mp(x2))\n",
        "        return [x1, x2, x3]\n",
        "\n",
        "    def dec(self, x):\n",
        "        x1, x2, x3 = x\n",
        "\n",
        "        x2 = torch.cat([x2, self._trans(x3)], dim=1)\n",
        "        x2 = self._dec1(x2)\n",
        "\n",
        "        x3 = torch.cat([x1, self._trans(x2)], dim=1)\n",
        "        x3 = self._dec1(x3)\n",
        "\n",
        "        return x3\n",
        "\n",
        "    def trunking(self, x):\n",
        "        return self._trunk2(self._trunk1(x))\n",
        "\n",
        "    def masking(self, x):\n",
        "        x = self.enc(x)\n",
        "        x = self.dec(x)\n",
        "        return torch.sigmoid(x)\n",
        "\n",
        "    def forward(self, x):\n",
        "        trunk = self.trunking(x)\n",
        "        mask = self.masking(x)\n",
        "        return (1 + mask) * trunk\n",
        "\n",
        "\n",
        "def attention(channels, block=BasicBlock, depth=-1):\n",
        "    if depth == 0:\n",
        "        return Attention0(channels, block)\n",
        "    elif depth == 1:\n",
        "        return Attention1(channels, block)\n",
        "    elif depth == 2:\n",
        "        return Attention2(channels, block)\n",
        "    else:\n",
        "        traceback.print_exc()\n",
        "        raise Exception(\"depth must be specified\")"
      ],
      "metadata": {
        "id": "n6-stN3hKhH3"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import traceback\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "\n",
        "def up_pooling(in_channels, out_channels, kernel_size=2, stride=2):\n",
        "    return nn.Sequential(\n",
        "        nn.ConvTranspose2d(\n",
        "            in_channels, out_channels, kernel_size=kernel_size, stride=stride\n",
        "        ),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(inplace=True),\n",
        "    )\n",
        "\n",
        "\n",
        "class Masking4(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, block=BasicBlock):\n",
        "        assert in_channels == out_channels\n",
        "        super(Masking4, self).__init__()\n",
        "        filters = [\n",
        "            in_channels,\n",
        "            in_channels * 2,\n",
        "            in_channels * 4,\n",
        "            in_channels * 8,\n",
        "            in_channels * 16,\n",
        "        ]\n",
        "\n",
        "        self.downsample1 = nn.Sequential(\n",
        "            conv1x1(filters[0], filters[1], 1),\n",
        "            nn.BatchNorm2d(filters[1]),\n",
        "        )\n",
        "\n",
        "        self.downsample2 = nn.Sequential(\n",
        "            conv1x1(filters[1], filters[2], 1),\n",
        "            nn.BatchNorm2d(filters[2]),\n",
        "        )\n",
        "\n",
        "        self.downsample3 = nn.Sequential(\n",
        "            conv1x1(filters[2], filters[3], 1),\n",
        "            nn.BatchNorm2d(filters[3]),\n",
        "        )\n",
        "\n",
        "        self.downsample4 = nn.Sequential(\n",
        "            conv1x1(filters[3], filters[4], 1),\n",
        "            nn.BatchNorm2d(filters[4]),\n",
        "        )\n",
        "\n",
        "        \"\"\"\n",
        "        self.conv1 = block(filters[0], filters[1], downsample=conv1x1(filters[0], filters[1], 1))\n",
        "        self.conv2 = block(filters[1], filters[2], downsample=conv1x1(filters[1], filters[2], 1))\n",
        "        self.conv3 = block(filters[2], filters[3], downsample=conv1x1(filters[2], filters[3], 1))\n",
        "        \"\"\"\n",
        "\n",
        "        self.conv1 = block(filters[0], filters[1], downsample=self.downsample1)\n",
        "        self.conv2 = block(filters[1], filters[2], downsample=self.downsample2)\n",
        "        self.conv3 = block(filters[2], filters[3], downsample=self.downsample3)\n",
        "        self.conv4 = block(filters[3], filters[4], downsample=self.downsample4)\n",
        "\n",
        "        self.down_pooling = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.downsample5 = nn.Sequential(\n",
        "            conv1x1(filters[4], filters[3], 1),\n",
        "            nn.BatchNorm2d(filters[3]),\n",
        "        )\n",
        "\n",
        "        self.downsample6 = nn.Sequential(\n",
        "            conv1x1(filters[3], filters[2], 1),\n",
        "            nn.BatchNorm2d(filters[2]),\n",
        "        )\n",
        "\n",
        "        self.downsample7 = nn.Sequential(\n",
        "            conv1x1(filters[2], filters[1], 1),\n",
        "            nn.BatchNorm2d(filters[1]),\n",
        "        )\n",
        "\n",
        "        self.downsample8 = nn.Sequential(\n",
        "            conv1x1(filters[1], filters[0], 1),\n",
        "            nn.BatchNorm2d(filters[0]),\n",
        "        )\n",
        "\n",
        "        \"\"\"\n",
        "        self.up_pool4 = up_pooling(filters[3], filters[2])\n",
        "        self.conv4 = block(filters[3], filters[2], downsample=conv1x1(filters[3], filters[2], 1))\n",
        "        self.up_pool5 = up_pooling(filters[2], filters[1])\n",
        "        self.conv5 = block(filters[2], filters[1], downsample=conv1x1(filters[2], filters[1], 1))\n",
        "\n",
        "        self.conv6 = block(filters[1], filters[0], downsample=conv1x1(filters[1], filters[0], 1))\n",
        "        \"\"\"\n",
        "\n",
        "        self.up_pool5 = up_pooling(filters[4], filters[3])\n",
        "        self.conv5 = block(filters[4], filters[3], downsample=self.downsample5)\n",
        "        self.up_pool6 = up_pooling(filters[3], filters[2])\n",
        "        self.conv6 = block(filters[3], filters[2], downsample=self.downsample6)\n",
        "        self.up_pool7 = up_pooling(filters[2], filters[1])\n",
        "        self.conv7 = block(filters[2], filters[1], downsample=self.downsample7)\n",
        "        self.conv8 = block(filters[1], filters[0], downsample=self.downsample8)\n",
        "\n",
        "        # init weight\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        # Zero-initialize the last BN in each residual branch,\n",
        "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
        "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, Bottleneck):\n",
        "                nn.init.constant_(m.bn3.weight, 0)\n",
        "            elif isinstance(m, BasicBlock):\n",
        "                nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.conv1(x)\n",
        "        p1 = self.down_pooling(x1)\n",
        "        x2 = self.conv2(p1)\n",
        "        p2 = self.down_pooling(x2)\n",
        "        x3 = self.conv3(p2)\n",
        "        p3 = self.down_pooling(x3)\n",
        "        x4 = self.conv4(p3)\n",
        "\n",
        "        x5 = self.up_pool5(x4)\n",
        "        x5 = torch.cat([x5, x3], dim=1)\n",
        "        x5 = self.conv5(x5)\n",
        "\n",
        "        x6 = self.up_pool6(x5)\n",
        "        x6 = torch.cat([x6, x2], dim=1)\n",
        "        x6 = self.conv6(x6)\n",
        "\n",
        "        x7 = self.up_pool7(x6)\n",
        "        x7 = torch.cat([x7, x1], dim=1)\n",
        "        x7 = self.conv7(x7)\n",
        "\n",
        "        x8 = self.conv8(x7)\n",
        "\n",
        "        output = torch.softmax(x8, dim=1)\n",
        "        # output = torch.sigmoid(x8)\n",
        "        return output\n",
        "\n",
        "\n",
        "class Masking3(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, block=BasicBlock):\n",
        "        assert in_channels == out_channels\n",
        "        super(Masking3, self).__init__()\n",
        "        filters = [in_channels, in_channels * 2, in_channels * 4, in_channels * 8]\n",
        "\n",
        "        self.downsample1 = nn.Sequential(\n",
        "            conv1x1(filters[0], filters[1], 1),\n",
        "            nn.BatchNorm2d(filters[1]),\n",
        "        )\n",
        "\n",
        "        self.downsample2 = nn.Sequential(\n",
        "            conv1x1(filters[1], filters[2], 1),\n",
        "            nn.BatchNorm2d(filters[2]),\n",
        "        )\n",
        "\n",
        "        self.downsample3 = nn.Sequential(\n",
        "            conv1x1(filters[2], filters[3], 1),\n",
        "            nn.BatchNorm2d(filters[3]),\n",
        "        )\n",
        "\n",
        "        \"\"\"\n",
        "        self.conv1 = block(filters[0], filters[1], downsample=conv1x1(filters[0], filters[1], 1))\n",
        "        self.conv2 = block(filters[1], filters[2], downsample=conv1x1(filters[1], filters[2], 1))\n",
        "        self.conv3 = block(filters[2], filters[3], downsample=conv1x1(filters[2], filters[3], 1))\n",
        "        \"\"\"\n",
        "\n",
        "        self.conv1 = block(filters[0], filters[1], downsample=self.downsample1)\n",
        "        self.conv2 = block(filters[1], filters[2], downsample=self.downsample2)\n",
        "        self.conv3 = block(filters[2], filters[3], downsample=self.downsample3)\n",
        "\n",
        "        self.down_pooling = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.downsample4 = nn.Sequential(\n",
        "            conv1x1(filters[3], filters[2], 1),\n",
        "            nn.BatchNorm2d(filters[2]),\n",
        "        )\n",
        "\n",
        "        self.downsample5 = nn.Sequential(\n",
        "            conv1x1(filters[2], filters[1], 1),\n",
        "            nn.BatchNorm2d(filters[1]),\n",
        "        )\n",
        "\n",
        "        self.downsample6 = nn.Sequential(\n",
        "            conv1x1(filters[1], filters[0], 1),\n",
        "            nn.BatchNorm2d(filters[0]),\n",
        "        )\n",
        "\n",
        "        \"\"\"\n",
        "        self.up_pool4 = up_pooling(filters[3], filters[2])\n",
        "        self.conv4 = block(filters[3], filters[2], downsample=conv1x1(filters[3], filters[2], 1))\n",
        "        self.up_pool5 = up_pooling(filters[2], filters[1])\n",
        "        self.conv5 = block(filters[2], filters[1], downsample=conv1x1(filters[2], filters[1], 1))\n",
        "\n",
        "        self.conv6 = block(filters[1], filters[0], downsample=conv1x1(filters[1], filters[0], 1))\n",
        "        \"\"\"\n",
        "\n",
        "        self.up_pool4 = up_pooling(filters[3], filters[2])\n",
        "        self.conv4 = block(filters[3], filters[2], downsample=self.downsample4)\n",
        "        self.up_pool5 = up_pooling(filters[2], filters[1])\n",
        "        self.conv5 = block(filters[2], filters[1], downsample=self.downsample5)\n",
        "\n",
        "        self.conv6 = block(filters[1], filters[0], downsample=self.downsample6)\n",
        "\n",
        "        # init weight\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        # Zero-initialize the last BN in each residual branch,\n",
        "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
        "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, Bottleneck):\n",
        "                nn.init.constant_(m.bn3.weight, 0)\n",
        "            elif isinstance(m, BasicBlock):\n",
        "                nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.conv1(x)\n",
        "        p1 = self.down_pooling(x1)\n",
        "        x2 = self.conv2(p1)\n",
        "        p2 = self.down_pooling(x2)\n",
        "        x3 = self.conv3(p2)\n",
        "\n",
        "        x4 = self.up_pool4(x3)\n",
        "        x4 = torch.cat([x4, x2], dim=1)\n",
        "\n",
        "        x4 = self.conv4(x4)\n",
        "\n",
        "        x5 = self.up_pool5(x4)\n",
        "        x5 = torch.cat([x5, x1], dim=1)\n",
        "        x5 = self.conv5(x5)\n",
        "\n",
        "        x6 = self.conv6(x5)\n",
        "\n",
        "        output = torch.softmax(x6, dim=1)\n",
        "        # output = torch.sigmoid(x6)\n",
        "        return output\n",
        "\n",
        "\n",
        "class Masking2(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, block=BasicBlock):\n",
        "        assert in_channels == out_channels\n",
        "        super(Masking2, self).__init__()\n",
        "        filters = [in_channels, in_channels * 2, in_channels * 4, in_channels * 8]\n",
        "\n",
        "        self.downsample1 = nn.Sequential(\n",
        "            conv1x1(filters[0], filters[1], 1),\n",
        "            nn.BatchNorm2d(filters[1]),\n",
        "        )\n",
        "\n",
        "        self.downsample2 = nn.Sequential(\n",
        "            conv1x1(filters[1], filters[2], 1),\n",
        "            nn.BatchNorm2d(filters[2]),\n",
        "        )\n",
        "\n",
        "        \"\"\"\n",
        "        self.conv1 = block(filters[0], filters[1], downsample=conv1x1(filters[0], filters[1], 1))\n",
        "        self.conv2 = block(filters[1], filters[2], downsample=conv1x1(filters[1], filters[2], 1))\n",
        "        \"\"\"\n",
        "        self.conv1 = block(filters[0], filters[1], downsample=self.downsample1)\n",
        "        self.conv2 = block(filters[1], filters[2], downsample=self.downsample2)\n",
        "\n",
        "        self.down_pooling = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.downsample3 = nn.Sequential(\n",
        "            conv1x1(filters[2], filters[1], 1),\n",
        "            nn.BatchNorm2d(filters[1]),\n",
        "        )\n",
        "\n",
        "        self.downsample4 = nn.Sequential(\n",
        "            conv1x1(filters[1], filters[0], 1),\n",
        "            nn.BatchNorm2d(filters[0]),\n",
        "        )\n",
        "\n",
        "        \"\"\"\n",
        "        self.up_pool3 = up_pooling(filters[2], filters[1])\n",
        "        self.conv3 = block(filters[2], filters[1], downsample=conv1x1(filters[2], filters[1], 1))\n",
        "        self.conv4 = block(filters[1], filters[0], downsample=conv1x1(filters[1], filters[0], 1))\n",
        "        \"\"\"\n",
        "        self.up_pool3 = up_pooling(filters[2], filters[1])\n",
        "        self.conv3 = block(filters[2], filters[1], downsample=self.downsample3)\n",
        "        self.conv4 = block(filters[1], filters[0], downsample=self.downsample4)\n",
        "\n",
        "        # init weight\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        # Zero-initialize the last BN in each residual branch,\n",
        "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
        "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, Bottleneck):\n",
        "                nn.init.constant_(m.bn3.weight, 0)\n",
        "            elif isinstance(m, BasicBlock):\n",
        "                nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.conv1(x)\n",
        "        p1 = self.down_pooling(x1)\n",
        "        x2 = self.conv2(p1)\n",
        "\n",
        "        x3 = self.up_pool3(x2)\n",
        "        x3 = torch.cat([x3, x1], dim=1)\n",
        "        x3 = self.conv3(x3)\n",
        "\n",
        "        x4 = self.conv4(x3)\n",
        "\n",
        "        output = torch.softmax(x4, dim=1)\n",
        "        # output = torch.sigmoid(x4)\n",
        "        return output\n",
        "\n",
        "\n",
        "class Masking1(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, block=BasicBlock):\n",
        "        assert in_channels == out_channels\n",
        "        super(Masking1, self).__init__()\n",
        "        filters = [in_channels, in_channels * 2, in_channels * 4, in_channels * 8]\n",
        "\n",
        "        self.downsample1 = nn.Sequential(\n",
        "            conv1x1(filters[0], filters[1], 1),\n",
        "            nn.BatchNorm2d(filters[1]),\n",
        "        )\n",
        "\n",
        "        self.conv1 = block(filters[0], filters[1], downsample=self.downsample1)\n",
        "\n",
        "        self.downsample2 = nn.Sequential(\n",
        "            conv1x1(filters[1], filters[0], 1),\n",
        "            nn.BatchNorm2d(filters[0]),\n",
        "        )\n",
        "\n",
        "        self.conv2 = block(filters[1], filters[0], downsample=self.downsample2)\n",
        "\n",
        "        # init weight\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        # Zero-initialize the last BN in each residual branch,\n",
        "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
        "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, Bottleneck):\n",
        "                nn.init.constant_(m.bn3.weight, 0)\n",
        "            elif isinstance(m, BasicBlock):\n",
        "                nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.conv1(x)\n",
        "        x2 = self.conv2(x1)\n",
        "        output = torch.softmax(x2, dim=1)\n",
        "        # output = torch.sigmoid(x2)\n",
        "        return output\n",
        "\n",
        "\n",
        "def masking(in_channels, out_channels, depth, block=BasicBlock):\n",
        "    if depth == 1:\n",
        "        return Masking1(in_channels, out_channels, block)\n",
        "    elif depth == 2:\n",
        "        return Masking2(in_channels, out_channels, block)\n",
        "    elif depth == 3:\n",
        "        return Masking3(in_channels, out_channels, block)\n",
        "    elif depth == 4:\n",
        "        return Masking4(in_channels, out_channels, block)\n",
        "    else:\n",
        "        traceback.print_exc()\n",
        "        raise Exception(\"depth need to be from 0-3\")"
      ],
      "metadata": {
        "id": "ceiYKrtNKzzP"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Código del modelo"
      ],
      "metadata": {
        "id": "DJCB_Z9hO9Sm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "AtHTdm98KVmG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "model_urls = {\n",
        "    \"resnet18\": \"https://download.pytorch.org/models/resnet18-5c106cde.pth\",\n",
        "    \"resnet34\": \"https://download.pytorch.org/models/resnet34-333f7ec4.pth\",\n",
        "    \"resnet50\": \"https://download.pytorch.org/models/resnet50-19c8e357.pth\",\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "class ResMasking(ResNet):\n",
        "    def __init__(self, weight_path):\n",
        "        super(ResMasking, self).__init__(\n",
        "            block=BasicBlock, layers=[3, 4, 6, 3], in_channels=3, num_classes=1000\n",
        "        )\n",
        "        #state_dict = torch.load(weight_path)['net']\n",
        "        #state_dict = torch.load('saved/checkpoints/resnet18_rot30_2019Nov05_17.44')['net']\n",
        "        state_dict = load_state_dict_from_url(model_urls['resnet34'], progress=True)\n",
        "        #self.load_state_dict(state_dict)\n",
        "\n",
        "        self.fc = nn.Linear(512, 7)\n",
        "\n",
        "        \"\"\"\n",
        "        # freeze all net\n",
        "        for m in self.parameters():\n",
        "            m.requires_grad = False\n",
        "        \"\"\"\n",
        "\n",
        "        self.mask1 = masking(64, 64, depth=4)\n",
        "        self.mask2 = masking(128, 128, depth=3)\n",
        "        self.mask3 = masking(256, 256, depth=2)\n",
        "        self.mask4 = masking(512, 512, depth=1)\n",
        "\n",
        "    def forward(self, x):  # 224\n",
        "        x = self.conv1(x)  # 112\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)  # 56\n",
        "\n",
        "        x = self.layer1(x)  # 56\n",
        "        m = self.mask1(x)\n",
        "        x = x * (1 + m)\n",
        "        # x = x * m\n",
        "\n",
        "        x = self.layer2(x)  # 28\n",
        "        m = self.mask2(x)\n",
        "        x = x * (1 + m)\n",
        "        # x = x * m\n",
        "\n",
        "        x = self.layer3(x)  # 14\n",
        "        m = self.mask3(x)\n",
        "        x = x * (1 + m)\n",
        "        # x = x * m\n",
        "\n",
        "        x = self.layer4(x)  # 7\n",
        "        m = self.mask4(x)\n",
        "        x = x * (1 + m)\n",
        "        # x = x * m\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResMasking50(ResNet):\n",
        "    def __init__(self, weight_path):\n",
        "        super(ResMasking50, self).__init__(\n",
        "            block=Bottleneck, layers=[3, 4, 6, 3], in_channels=3, num_classes=1000\n",
        "        )\n",
        "        # state_dict = torch.load(weight_path)['net']\n",
        "        state_dict = load_state_dict_from_url(model_urls[\"resnet50\"], progress=True)\n",
        "        self.load_state_dict(state_dict)\n",
        "\n",
        "        self.fc = nn.Linear(2048, 7)\n",
        "\n",
        "        \"\"\"\n",
        "        # freeze all net\n",
        "        for m in self.parameters():\n",
        "            m.requires_grad = False\n",
        "        \"\"\"\n",
        "\n",
        "        self.mask1 = masking(256, 256, depth=4)\n",
        "        self.mask2 = masking(512, 512, depth=3)\n",
        "        self.mask3 = masking(1024, 1024, depth=2)\n",
        "        self.mask4 = masking(2048, 2048, depth=1)\n",
        "\n",
        "    def forward(self, x):  # 224\n",
        "        x = self.conv1(x)  # 112\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)  # 56\n",
        "\n",
        "        x = self.layer1(x)  # 56\n",
        "        m = self.mask1(x)\n",
        "        x = x * (1 + m)\n",
        "\n",
        "        x = self.layer2(x)  # 28\n",
        "        m = self.mask2(x)\n",
        "        x = x * (1 + m)\n",
        "\n",
        "        x = self.layer3(x)  # 14\n",
        "        m = self.mask3(x)\n",
        "        x = x * (1 + m)\n",
        "\n",
        "        x = self.layer4(x)  # 7\n",
        "        m = self.mask4(x)\n",
        "        x = x * (1 + m)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# def resmasking(in_channels, num_classes, weight_path='saved/checkpoints/resnet18_rot30_2019Nov05_17.44'):\n",
        "#     return ResMasking(weight_path)\n",
        "\n",
        "\n",
        "def resmasking(in_channels, num_classes, weight_path=\"\"):\n",
        "    return ResMasking(weight_path)\n",
        "\n",
        "\n",
        "def resmasking50_dropout1(in_channels, num_classes, weight_path=\"\"):\n",
        "    model = ResMasking50(weight_path)\n",
        "    model.fc = nn.Sequential(nn.Dropout(0.4), nn.Linear(2048, num_classes))\n",
        "    return model\n",
        "\n",
        "\n",
        "def resmasking_dropout1(in_channels=3, num_classes=7, weight_path=\"\"):\n",
        "    model = ResMasking(weight_path)\n",
        "    model.fc = nn.Sequential(\n",
        "        nn.Dropout(0.4),\n",
        "        nn.Linear(512, 7)\n",
        "        # nn.Linear(512, num_classes)\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "def resmasking_dropout2(in_channels, num_classes, weight_path=\"\"):\n",
        "    model = ResMasking(weight_path)\n",
        "\n",
        "    model.fc = nn.Sequential(\n",
        "        nn.Linear(512, 128),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(p=0.5),\n",
        "        nn.Linear(128, 7),\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "def resmasking_dropout3(in_channels, num_classes, weight_path=\"\"):\n",
        "    model = ResMasking(weight_path)\n",
        "\n",
        "    model.fc = nn.Sequential(\n",
        "        nn.Linear(512, 512),\n",
        "        nn.ReLU(True),\n",
        "        nn.Dropout(),\n",
        "        nn.Linear(512, 128),\n",
        "        nn.ReLU(True),\n",
        "        nn.Dropout(),\n",
        "        nn.Linear(128, 7),\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "def resmasking_dropout4(in_channels, num_classes, weight_path=\"\"):\n",
        "    model = ResMasking(weight_path)\n",
        "\n",
        "    model.fc = nn.Sequential(\n",
        "        nn.Linear(512, 128),\n",
        "        nn.ReLU(True),\n",
        "        nn.Dropout(),\n",
        "        nn.Linear(128, 128),\n",
        "        nn.ReLU(True),\n",
        "        nn.Dropout(),\n",
        "        nn.Linear(128, 7),\n",
        "    )\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Instanciamos el modelo"
      ],
      "metadata": {
        "id": "SCeIOwj8MoQp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = resmasking_dropout1(in_channels=3, num_classes=7, weight_path=\"resmasking_dropout1_rot30_2019Nov17_14.33\")\n"
      ],
      "metadata": {
        "id": "HBk3XJNaMY0-"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Cargamos una imagen"
      ],
      "metadata": {
        "id": "Fp9qnqGfMrOv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# Carga de una imagen de prueba\n",
        "image_path = \"Anger_David_on_c_s.jpg\"\n",
        "image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "input_image = transform(image).unsqueeze(0)\n"
      ],
      "metadata": {
        "id": "Ym3hGKjjMhy2"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Realizamos predicción"
      ],
      "metadata": {
        "id": "j9VeCMDqM3E0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    output = model(input_image)\n"
      ],
      "metadata": {
        "id": "pSCAF5SJMk3w"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Mostramos clase predicha"
      ],
      "metadata": {
        "id": "-6ExptRBNNq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_mapping = {0: \"neutral\", 1: \"angry\", 2: \"disgust\", 3: \"fear\", 4: \"happy\", 5: \"sad\", 6: \"surprise\"}\n",
        "predicted_class_index = torch.argmax(output).item()\n",
        "predicted_class_name = class_mapping[predicted_class_index]\n",
        "\n",
        "print(\"La clase predicha es:\", predicted_class_name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bEz-uVXVMlXf",
        "outputId": "8dfb8ce8-2a49-4e3d-8858-e3ca3cd7806e"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "La clase predicha es: sad\n"
          ]
        }
      ]
    }
  ]
}